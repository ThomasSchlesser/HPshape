{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Generation With N-Grams\n",
    "\n",
    "N-Grams are a probability distribution on strings for a given language. For example, in French, the word \"xyloqsjdfkljsdfqmskfdl\" has a smaller probablity of being a valid word than \"xylophone\". <big>How much smaller?</big>\n",
    "\n",
    "### 1. The trainable model\n",
    "\n",
    "Our model will predict the likeliness of a token given the $n$ preceding tokens. Tokens could be letters or words **but an end token is needed.** In python this does not exist, so we'll have to add it at the end of strings ourselves. We will also add $n$ end-tokens at the beginning of a string, and start analyzing the string when at the first non ending character. Let $\\mathcal{T}$ be the set of tokens. Here we will make an *updateable* model, that estimates the $|\\mathcal{T}|^{n+1}$ parameters of the model. The parameters are the probability that an $n$-gram (sequence of $n$ tokens) is followed by any given token, for all tokens and for all $n$-grams. Let $f:\\mathcal{T}\\times \\mathcal{T}^{n} \\to [0,1]$ be  defined as $$\\forall t \\in \\mathcal{T}, w \\in \\mathcal{T}^n, f(t,w) := P(t | w).$$ Note that $$f(t,w)\\approx \\frac{\\char\"0023 w.t}{\\char\"0023 w},$$ where $\\char\"0023$ stands for \"the count of\", and $.$ stands for string concatenation, aka \"followed by\". This model stores the parameters as :\n",
    "\n",
    "- The raw counts of each $n+1$-gram as a $(|\\mathcal{T}|,|\\mathcal{T}|,|\\mathcal{T}|,...,|\\mathcal{T}|)-$ shaped ($n+1$ repetitions) array. \n",
    "- The raw counts of each token, stored as a $(|\\mathcal{T}|)-$ shaped array.\n",
    "\n",
    "These two elements make it possible to reconstitute a $|\\mathcal{T}|\\times |\\mathcal{T}|^n$ matrix, containing the associated probabilities. This model will be updated with new strings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ngram:\n",
    "\n",
    "\n",
    "    def __init__(self, n,tokenizer,end_token,alphabet:list):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.n = n\n",
    "\n",
    "        self.alphabet:np.array = np.array(alphabet+[end_token])# initialize alphabet as an array of all possible tokens, **including the end token**. \n",
    "        self.raw_count_of_each_token = np.zeros(shape=self.alphabet.shape)\n",
    "\n",
    "        self.ngrams:np.array = ... # initialize all possible ngrams by using the alphabet. \n",
    "\n",
    "        self.raw_count_of_each_n_plus_one_gram = np.zeros([len(alphabet)+1] * (n+1))\n",
    "\n",
    "    def update_from_file(self,path:str):\n",
    "        ... # Extract the list, and use update_from_list\n",
    "\n",
    "    def update_from_list(self,l:list):\n",
    "        ... # for each word in the list, \n",
    "            # for each token in the word, starting at token index n+1,\n",
    "            # Update the respective raw counts.\n",
    "\n",
    "    def probability(self, s:str):\n",
    "        prod = 1 # the initial product value is 1\n",
    "        # tokenize s.\n",
    "        # for each token t in s, starting at index n+1, including the ending token,\n",
    "        # given the preceding n-gram (:=w)\n",
    "        # prod *= f(t,w)\n",
    "        \n",
    "        return prod\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code should now work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "END_TOKEN = '\\x00'\n",
    "PATH_TO_STRINGS_CSV = ...\n",
    "ALPHABET = [c for c in \"abcdefghijklmnopqrstuvwxyz\"]\n",
    "\n",
    "def t(s:str): # s, if it is a finished string, must be ended with END TOKENS. It must also be padded with n end tokens to be able to work with words of length less than n.\n",
    "    # example tokenizer\n",
    "    l = [c for c in s] # tokenize by char\n",
    "    return l\n",
    "    \n",
    "model_fr = Ngram(n=N,tokenizer=t,end_token=END_TOKEN,alphabet=ALPHABET) # generate new model from constructor, choosing n and a tokenizer (it must also be possible to `load` a model from csv)\n",
    "\n",
    "model_fr.update_from_list([\"chaussettes\",\"chien\"])# train the model using a list of strings\n",
    "model_fr.update_from_file(PATH_TO_STRINGS_CSV)# train the model using a csv file containing correct strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fr.probability(\"chaussette\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now implement storing and loading. Note that the following code is not as important as what comes before. If you are implementing this, what you did works ok and you don't feel like implementing stores and loads, just go to part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_MODEL_CSV = ...\n",
    "\n",
    "def store(self,path:str):\n",
    "    ...#the file must contain:\n",
    "    # - the raw_count_of_each_n_plus_one_gram\n",
    "    # - the raw_count_of_each_token\n",
    "    # - n\n",
    "    # loaded models will GUESS the type of tokenization to be by-letter\n",
    "\n",
    "\n",
    "def load_from(path:str)->Ngram:\n",
    "    ...\n",
    "\n",
    "Ngram.store = store\n",
    "Ngram.load_from = load_from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following should now be possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fr.store(PATH_TO_MODEL_CSV) # store the model in a csv file\n",
    "... #do stuff with the model\n",
    "model_fr = Ngram.load_from(PATH_TO_MODEL_CSV) # restart the model to where it was before, from memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The end product of this first part is to determine the probability of any given string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Word generation\n",
    "\n",
    "Time to generate new words! To do so, we will need strings of tokens that have their beginning padded with $n$ end-tokens. We will look at the most likely token to follow in the table obtained before.\n",
    "\n",
    "First, let us define a function that finds the most likely following token, given an $n$-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "198d2f867f2b2890f2b4b4b250eaab6c61028a7cc43b500b1c12fee0b3f13cfc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
